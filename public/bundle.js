/*
 * ATTENTION: The "eval" devtool has been used (maybe by default in mode: "development").
 * This devtool is neither made for production nor for readable output files.
 * It uses "eval()" calls to create a separate source file in the browser devtools.
 * If you are trying to read the output file, select a different devtool (https://webpack.js.org/configuration/devtool/)
 * or disable the default devtool with "devtool: false".
 * If you are looking for production-ready output files, see mode: "production" (https://webpack.js.org/configuration/mode/).
 */
/******/ (() => { // webpackBootstrap
/******/ 	"use strict";
/******/ 	var __webpack_modules__ = ({

/***/ "./js/api/chat.js":
/*!************************!*\
  !*** ./js/api/chat.js ***!
  \************************/
/***/ ((__unused_webpack_module, __webpack_exports__, __webpack_require__) => {

eval("__webpack_require__.r(__webpack_exports__);\n/* harmony export */ __webpack_require__.d(__webpack_exports__, {\n/* harmony export */   ChatService: () => (/* binding */ ChatService)\n/* harmony export */ });\n/**\n * Service de chat avec l'API\n */\nclass ChatService {\n    constructor() {\n        this.messageHistory = [];\n        this.currentAbortController = null;\n    }\n\n    /**\n     * Envoie un message à l'API et obtient une réponse\n     * @param {string} message - Message de l'utilisateur\n     * @param {string} apiKey - Clé API Groq\n     * @returns {Promise<string>} Réponse du modèle\n     */\n    async sendMessage(message, apiKey) {\n        if (!apiKey) {\n            throw new Error(\"Aucune clé API fournie\");\n        }\n\n        // Ajouter le message à l'historique\n        this.messageHistory.push({\n            role: \"user\",\n            content: message\n        });\n\n        // Créer un AbortController pour pouvoir annuler la requête\n        this.currentAbortController = new AbortController();\n        const signal = this.currentAbortController.signal;\n\n        try {\n            const response = await fetch(\n                \"https://api.groq.com/openai/v1/chat/completions\",\n                {\n                    method: \"POST\",\n                    headers: {\n                        Authorization: `Bearer ${apiKey}`,\n                        \"Content-Type\": \"application/json\",\n                    },\n                    body: JSON.stringify({\n                        messages: this.messageHistory,\n                        model: \"gemma2-9b-it\",\n                    }),\n                    signal: signal,\n                }\n            );\n\n            if (!response.ok) {\n                throw new Error(`Erreur de chat: ${response.status} - ${response.statusText}`);\n            }\n\n            const result = await response.json();\n            \n            // Ajouter la réponse à l'historique\n            if (result.choices && result.choices[0] && result.choices[0].message) {\n                this.messageHistory.push(result.choices[0].message);\n                return result.choices[0].message.content;\n            } else {\n                throw new Error(\"Format de réponse invalide\");\n            }\n        } finally {\n            this.currentAbortController = null;\n        }\n    }\n\n    /**\n     * Annule la requête de chat en cours\n     */\n    cancel() {\n        if (this.currentAbortController) {\n            this.currentAbortController.abort();\n            this.currentAbortController = null;\n        }\n    }\n\n    /**\n     * Réinitialise l'historique des messages\n     */\n    clearHistory() {\n        this.messageHistory = [];\n    }\n\n    /**\n     * Obtient l'historique des messages\n     * @returns {Array} Historique des messages\n     */\n    getHistory() {\n        return [...this.messageHistory];\n    }\n}\n\n//# sourceURL=webpack://voice-test-html/./js/api/chat.js?");

/***/ }),

/***/ "./js/api/transcription.js":
/*!*********************************!*\
  !*** ./js/api/transcription.js ***!
  \*********************************/
/***/ ((__unused_webpack_module, __webpack_exports__, __webpack_require__) => {

eval("__webpack_require__.r(__webpack_exports__);\n/* harmony export */ __webpack_require__.d(__webpack_exports__, {\n/* harmony export */   TranscriptionService: () => (/* binding */ TranscriptionService)\n/* harmony export */ });\n/* harmony import */ var _audio_encoder_js__WEBPACK_IMPORTED_MODULE_0__ = __webpack_require__(/*! ../audio/encoder.js */ \"./js/audio/encoder.js\");\n\n\n/**\n * Service de transcription audio\n */\nclass TranscriptionService {\n    constructor() {\n        this.currentAbortController = null;\n    }\n\n    /**\n     * Transcrit un segment audio\n     * @param {Float32Array} audioData - Données audio\n     * @param {string} apiKey - Clé API Groq\n     * @returns {Promise<string>} Texte transcrit\n     */\n    async transcribe(audioData, apiKey) {\n        if (!apiKey) {\n            throw new Error(\"Aucune clé API fournie\");\n        }\n\n        // Créer un AbortController pour pouvoir annuler la requête\n        this.currentAbortController = new AbortController();\n        const signal = this.currentAbortController.signal;\n\n        try {\n            // Convertir l'audio en WAV\n            const wavBlob = _audio_encoder_js__WEBPACK_IMPORTED_MODULE_0__.AudioEncoder.createWavBlob(audioData);\n            \n            // Créer un FormData pour l'API\n            const formData = new FormData();\n            formData.append('file', wavBlob, 'audio.wav');\n            formData.append('model', 'whisper-large-v3-turbo');\n            formData.append('temperature', '0');\n            formData.append('response_format', 'json');\n            formData.append('language', 'fr');\n            \n            // Envoyer à l'API\n            const response = await fetch(\n                \"https://api.groq.com/openai/v1/audio/transcriptions\",\n                {\n                    method: \"POST\",\n                    headers: {\n                        Authorization: `Bearer ${apiKey}`,\n                    },\n                    body: formData,\n                    signal: signal,\n                }\n            );\n\n            if (!response.ok) {\n                throw new Error(`Erreur de transcription: ${response.status} - ${response.statusText}`);\n            }\n\n            const result = await response.json();\n            return result.text;\n        } finally {\n            this.currentAbortController = null;\n        }\n    }\n\n    /**\n     * Annule la requête de transcription en cours\n     */\n    cancel() {\n        if (this.currentAbortController) {\n            this.currentAbortController.abort();\n            this.currentAbortController = null;\n        }\n    }\n}\n\n//# sourceURL=webpack://voice-test-html/./js/api/transcription.js?");

/***/ }),

/***/ "./js/api/tts.js":
/*!***********************!*\
  !*** ./js/api/tts.js ***!
  \***********************/
/***/ ((__unused_webpack_module, __webpack_exports__, __webpack_require__) => {

eval("__webpack_require__.r(__webpack_exports__);\n/* harmony export */ __webpack_require__.d(__webpack_exports__, {\n/* harmony export */   TTSService: () => (/* binding */ TTSService)\n/* harmony export */ });\n// js/api/tts.js\nclass TTSService {\n    constructor() {\n        this.currentAbortController = null;\n        this.audioElement = document.getElementById('tts-player');\n        this.isSpeaking = false;\n        this.onSpeakingStarted = null;\n        this.onSpeakingEnded = null;\n        this.onSpeakingInterrupted = null;\n\n        // S'assurer que nous avons trouvé l'élément audio\n        if (!this.audioElement) {\n            console.error(\"Élément audio non trouvé dans le DOM, création d'un élément temporaire\");\n            this.audioElement = document.createElement('audio');\n            this.audioElement.id = 'tts-player';\n            document.body.appendChild(this.audioElement);\n        }\n\n        // Ajouter les écouteurs d'événements\n        this.setupAudioListeners();\n    }\n\n    setupAudioListeners() {\n        // Quand l'audio commence\n        this.audioElement.onplay = () => {\n            console.log(\"TTS: lecture démarrée\");\n            this.isSpeaking = true;\n            if (this.onSpeakingStarted) this.onSpeakingStarted();\n        };\n        \n        // Quand l'audio se termine naturellement\n        this.audioElement.onended = () => {\n            console.log(\"TTS: lecture terminée\");\n            this.isSpeaking = false;\n            if (this.onSpeakingEnded) this.onSpeakingEnded();\n        };\n        \n        // En cas d'erreur\n        this.audioElement.onerror = (e) => {\n            console.error(\"TTS: erreur de lecture\", e);\n            this.isSpeaking = false;\n            if (this.onSpeakingInterrupted) this.onSpeakingInterrupted();\n        };\n\n        // En cas de pause\n        this.audioElement.onpause = () => {\n            if (this.isSpeaking) {\n                console.log(\"TTS: lecture mise en pause\");\n                this.isSpeaking = false;\n                if (this.onSpeakingInterrupted) this.onSpeakingInterrupted();\n            }\n        };\n    }\n\n    async speak(text) {\n        if (!text) {\n            return Promise.resolve();\n        }\n\n        // Créer un AbortController pour pouvoir annuler la requête\n        this.currentAbortController = new AbortController();\n        const signal = this.currentAbortController.signal;\n\n        try {\n            // Afficher l'état dans la console pour debugging\n            console.log(\"TTS: Envoi de la requête de synthèse vocale\");\n            \n            const response = await fetch(\n                \"https://chatbot-20102024-8c94bbb4eddf.herokuapp.com/synthesize\",\n                {\n                    method: \"POST\",\n                    headers: { \"Content-Type\": \"application/json\" },\n                    body: JSON.stringify({\n                        text: text,\n                        voice: \"fr-FR-DeniseNeural\",\n                    }),\n                    signal: signal,\n                }\n            );\n\n            if (!response.ok) {\n                throw new Error(`Erreur de synthèse vocale: ${response.status} - ${response.statusText}`);\n            }\n\n            const audioBlob = await response.blob();\n            const audioUrl = URL.createObjectURL(audioBlob);\n            \n            // Nettoyer l'URL précédente si elle existe\n            if (this.audioElement.src) {\n                URL.revokeObjectURL(this.audioElement.src);\n            }\n            \n            console.log(\"TTS: Audio reçu, configuration du lecteur\");\n            \n            // Définir la nouvelle source\n            this.audioElement.src = audioUrl;\n            \n            // Pour debugging, montrer l'élément audio\n            // this.audioElement.style.display = 'block';\n            \n            // Lancer la lecture et retourner une promesse\n            console.log(\"TTS: Tentative de lecture\");\n            return new Promise((resolve, reject) => {\n                // Un timeout de sécurité\n                const safetyTimeout = setTimeout(() => {\n                    console.warn(\"TTS: Timeout de sécurité déclenché\");\n                    this.isSpeaking = false;\n                    if (this.onSpeakingEnded) this.onSpeakingEnded();\n                    resolve();\n                }, 30000);\n                \n                // Écouteur temporaire pour résoudre la promesse\n                const onEndedOnce = () => {\n                    clearTimeout(safetyTimeout);\n                    this.audioElement.removeEventListener('ended', onEndedOnce);\n                    resolve();\n                };\n                \n                this.audioElement.addEventListener('ended', onEndedOnce);\n                \n                // Gestion des erreurs\n                const onErrorOnce = (e) => {\n                    clearTimeout(safetyTimeout);\n                    this.audioElement.removeEventListener('error', onErrorOnce);\n                    console.error(\"TTS: Erreur lors de la lecture\", e);\n                    resolve(); // Résoudre plutôt que rejeter pour ne pas bloquer le flux\n                };\n                \n                this.audioElement.addEventListener('error', onErrorOnce);\n                \n                // Lancer la lecture\n                this.audioElement.play().catch(error => {\n                    clearTimeout(safetyTimeout);\n                    console.error(\"TTS: Impossible de démarrer la lecture\", error);\n                    this.isSpeaking = false;\n                    if (this.onSpeakingInterrupted) this.onSpeakingInterrupted();\n                    resolve(); // Résoudre quand même pour éviter de bloquer\n                });\n            });\n        } catch (error) {\n            console.error(\"TTS: Erreur générale\", error);\n            this.isSpeaking = false;\n            if (this.onSpeakingInterrupted) this.onSpeakingInterrupted();\n            return Promise.resolve(); // Résoudre quand même pour éviter de bloquer\n        } finally {\n            this.currentAbortController = null;\n        }\n    }\n\n    cancel() {\n        // Annuler la requête en cours\n        if (this.currentAbortController) {\n            this.currentAbortController.abort();\n            this.currentAbortController = null;\n        }\n        \n        // Arrêter l'audio en cours\n        if (this.audioElement) {\n            this.audioElement.pause();\n            this.audioElement.currentTime = 0;\n            \n            // Indiquer que la parole a été interrompue\n            if (this.isSpeaking) {\n                console.log(\"TTS: Lecture interrompue manuellement\");\n                this.isSpeaking = false;\n                if (this.onSpeakingInterrupted) this.onSpeakingInterrupted();\n            }\n        }\n    }\n}\n\n//# sourceURL=webpack://voice-test-html/./js/api/tts.js?");

/***/ }),

/***/ "./js/audio/encoder.js":
/*!*****************************!*\
  !*** ./js/audio/encoder.js ***!
  \*****************************/
/***/ ((__unused_webpack_module, __webpack_exports__, __webpack_require__) => {

eval("__webpack_require__.r(__webpack_exports__);\n/* harmony export */ __webpack_require__.d(__webpack_exports__, {\n/* harmony export */   AudioEncoder: () => (/* binding */ AudioEncoder)\n/* harmony export */ });\n/**\n * Classe pour l'encodage audio\n */\nclass AudioEncoder {\n    /**\n     * Convertit un tableau Float32Array en PCM 16 bits\n     * @param {DataView} output - Vue de données de sortie\n     * @param {number} offset - Offset dans la vue de données\n     * @param {Float32Array} input - Données audio en Float32\n     */\n    static floatTo16BitPCM(output, offset, input) {\n        for (let i = 0; i < input.length; i++, offset += 2) {\n            const s = Math.max(-1, Math.min(1, input[i]));\n            output.setInt16(offset, s < 0 ? s * 0x8000 : s * 0x7FFF, true);\n        }\n    }\n\n    /**\n     * Écrit une chaîne dans une DataView\n     * @param {DataView} view - Vue de données\n     * @param {number} offset - Position de départ\n     * @param {string} string - Chaîne à écrire\n     */\n    static writeString(view, offset, string) {\n        for (let i = 0; i < string.length; i++) {\n            view.setUint8(offset + i, string.charCodeAt(i));\n        }\n    }\n\n    /**\n     * Encode des données audio en format WAV\n     * @param {Float32Array} samples - Échantillons audio\n     * @param {number} sampleRate - Taux d'échantillonnage\n     * @returns {ArrayBuffer} Données WAV encodées\n     */\n    static encodeWAV(samples, sampleRate = 16000) {\n        const buffer = new ArrayBuffer(44 + samples.length * 2);\n        const view = new DataView(buffer);\n\n        /* RIFF identifier */\n        this.writeString(view, 0, 'RIFF');\n        /* RIFF chunk length */\n        view.setUint32(4, 36 + samples.length * 2, true);\n        /* RIFF type */\n        this.writeString(view, 8, 'WAVE');\n        /* format chunk identifier */\n        this.writeString(view, 12, 'fmt ');\n        /* format chunk length */\n        view.setUint32(16, 16, true);\n        /* sample format (raw) */\n        view.setUint16(20, 1, true);\n        /* channel count */\n        view.setUint16(22, 1, true);\n        /* sample rate */\n        view.setUint32(24, sampleRate, true);\n        /* byte rate (sample rate * block align) */\n        view.setUint32(28, sampleRate * 2, true);\n        /* block align (channel count * bytes per sample) */\n        view.setUint16(32, 2, true);\n        /* bits per sample */\n        view.setUint16(34, 16, true);\n        /* data chunk identifier */\n        this.writeString(view, 36, 'data');\n        /* data chunk length */\n        view.setUint32(40, samples.length * 2, true);\n\n        this.floatTo16BitPCM(view, 44, samples);\n\n        return buffer;\n    }\n    \n    /**\n     * Convertit un ArrayBuffer en Blob WAV\n     * @param {Float32Array} audioData - Données audio\n     * @returns {Blob} Blob WAV\n     */\n    static createWavBlob(audioData) {\n        const wavBuffer = this.encodeWAV(audioData);\n        return new Blob([wavBuffer], { type: 'audio/wav' });\n    }\n}\n\n//# sourceURL=webpack://voice-test-html/./js/audio/encoder.js?");

/***/ }),

/***/ "./js/audio/recorder.js":
/*!******************************!*\
  !*** ./js/audio/recorder.js ***!
  \******************************/
/***/ ((__unused_webpack_module, __webpack_exports__, __webpack_require__) => {

eval("__webpack_require__.r(__webpack_exports__);\n/* harmony export */ __webpack_require__.d(__webpack_exports__, {\n/* harmony export */   AudioRecorder: () => (/* binding */ AudioRecorder)\n/* harmony export */ });\n/**\n * Gestion de l'enregistrement audio manuel\n */\nclass AudioRecorder {\n    /**\n     * @param {Object} callbacks - Fonctions de callback\n     */\n    constructor(callbacks = {}) {\n        this.mediaRecorder = null;\n        this.mediaStream = null;\n        this.audioChunks = [];\n        this.recording = false;\n        this.callbacks = callbacks;\n    }\n\n    /**\n     * Initialise l'enregistreur\n     * @returns {Promise<boolean>} Succès ou échec de l'initialisation\n     */\n    async initialize() {\n        try {\n            // Demander l'accès au microphone\n            this.mediaStream = await navigator.mediaDevices.getUserMedia({ \n                audio: {\n                    sampleRate: 16000,\n                    channelCount: 1,\n                    echoCancellation: true,\n                    noiseSuppression: true\n                } \n            });\n            \n            // Initialiser le MediaRecorder\n            const options = {\n                mimeType: 'audio/webm;codecs=opus',\n                audioBitsPerSecond: 16000\n            };\n            \n            try {\n                this.mediaRecorder = new MediaRecorder(this.mediaStream, options);\n            } catch (e) {\n                console.warn(\"Format audio spécifié non pris en charge, utilisation du format par défaut\");\n                this.mediaRecorder = new MediaRecorder(this.mediaStream);\n            }\n            \n            // Configurer les événements du MediaRecorder\n            this.mediaRecorder.ondataavailable = (event) => {\n                if (event.data.size > 0) {\n                    this.audioChunks.push(event.data);\n                }\n            };\n            \n            this.mediaRecorder.onstop = async () => {\n                console.log(\"Enregistrement terminé, traitement en cours...\");\n                \n                if (this.audioChunks.length === 0) {\n                    console.warn(\"Pas de données audio enregistrées\");\n                    return;\n                }\n                \n                try {\n                    // Convertir les chunks en blob\n                    const audioBlob = new Blob(this.audioChunks, { type: 'audio/webm;codecs=opus' });\n                    console.log(\"Taille du blob audio:\", audioBlob.size, \"octets\");\n                    \n                    // Convertir le Blob en ArrayBuffer\n                    const arrayBuffer = await audioBlob.arrayBuffer();\n                    console.log(\"Taille de l'ArrayBuffer:\", arrayBuffer.byteLength, \"octets\");\n                    \n                    // Décoder l'audio\n                    const audioContext = new (window.AudioContext || window.webkitAudioContext)();\n                    \n                    // Assurez-vous que le décodage se passe bien\n                    let audioBuffer;\n                    try {\n                        audioBuffer = await audioContext.decodeAudioData(arrayBuffer);\n                        console.log(\"Audio décodé avec succès, durée:\", audioBuffer.duration, \"s, canaux:\", audioBuffer.numberOfChannels);\n                    } catch (decodeError) {\n                        console.error(\"Erreur lors du décodage audio:\", decodeError);\n                        // Essayer une approche alternative avec un format différent\n                        const alternativeBlob = new Blob(this.audioChunks, { type: 'audio/wav' });\n                        const alternativeBuffer = await alternativeBlob.arrayBuffer();\n                        audioBuffer = await audioContext.decodeAudioData(alternativeBuffer);\n                    }\n                    \n                    // Extraire les données audio en tant que Float32Array\n                    const audioData = audioBuffer.getChannelData(0);\n                    console.log(\"Données audio extraites, longueur:\", audioData.length, \"échantillons\");\n                    console.log(\"Échantillon de valeurs:\", audioData.slice(0, 5));\n                    \n                    // Vérifier si nous devons rééchantillonner pour 16000Hz\n                    const targetSampleRate = 16000;\n                    let finalAudioData = audioData;\n                    \n                    if (audioBuffer.sampleRate !== targetSampleRate) {\n                        console.log(\"Rééchantillonnage de\", audioBuffer.sampleRate, \"Hz à\", targetSampleRate, \"Hz\");\n                        finalAudioData = this.resampleAudio(audioData, audioBuffer.sampleRate, targetSampleRate);\n                        console.log(\"Audio rééchantillonné, nouvelle longueur:\", finalAudioData.length);\n                    }\n                    \n                    // Réinitialiser les chunks pour le prochain enregistrement\n                    this.audioChunks = [];\n                    \n                    // Appeler le callback avec les données audio\n                    if (this.callbacks.onRecordingComplete) {\n                        this.callbacks.onRecordingComplete(finalAudioData);\n                    }\n                } catch (error) {\n                    console.error(\"Erreur lors du traitement de l'audio enregistré:\", error);\n                    if (this.callbacks.onError) {\n                        this.callbacks.onError(error);\n                    }\n                }\n            };\n            \n            return true;\n        } catch (error) {\n            console.error(\"Erreur lors de l'initialisation de l'enregistreur:\", error);\n            if (this.callbacks.onError) {\n                this.callbacks.onError(error);\n            }\n            return false;\n        }\n    }\n\n    /**\n     * Commence l'enregistrement\n     */\n    startRecording() {\n        if (!this.mediaRecorder || this.recording) {\n            return;\n        }\n        \n        this.audioChunks = [];\n        \n        try {\n            this.mediaRecorder.start(100); // Recueillir des chunks toutes les 100ms\n        } catch (error) {\n            console.error(\"Erreur au démarrage de l'enregistrement:\", error);\n            // Essayer sans options si l'erreur se produit\n            this.mediaRecorder.start();\n        }\n        \n        this.recording = true;\n        \n        console.log(\"Enregistrement démarré\");\n        \n        if (this.callbacks.onRecordingStart) {\n            this.callbacks.onRecordingStart();\n        }\n    }\n\n    /**\n     * Arrête l'enregistrement\n     */\n    stopRecording() {\n        if (!this.mediaRecorder || !this.recording) {\n            return;\n        }\n        \n        this.mediaRecorder.stop();\n        this.recording = false;\n        \n        console.log(\"Enregistrement arrêté\");\n        \n        if (this.callbacks.onRecordingStop) {\n            this.callbacks.onRecordingStop();\n        }\n    }\n\n    /**\n     * Libère les ressources\n     */\n    dispose() {\n        if (this.mediaStream) {\n            this.mediaStream.getTracks().forEach(track => track.stop());\n        }\n        \n        this.mediaRecorder = null;\n        this.mediaStream = null;\n        this.audioChunks = [];\n        this.recording = false;\n    }\n\n    /**\n     * Vérifie si l'enregistrement est en cours\n     * @returns {boolean} État de l'enregistrement\n     */\n    isRecording() {\n        return this.recording;\n    }\n\n    /**\n     * Rééchantillonne l'audio à un taux d'échantillonnage cible\n     * @param {Float32Array} audioData - Données audio source\n     * @param {number} sourceSampleRate - Taux d'échantillonnage source\n     * @param {number} targetSampleRate - Taux d'échantillonnage cible\n     * @returns {Float32Array} - Données audio rééchantillonnées\n     */\n    resampleAudio(audioData, sourceSampleRate, targetSampleRate) {\n        if (sourceSampleRate === targetSampleRate) {\n            return audioData;\n        }\n        \n        const ratio = sourceSampleRate / targetSampleRate;\n        const newLength = Math.round(audioData.length / ratio);\n        const result = new Float32Array(newLength);\n        \n        for (let i = 0; i < newLength; i++) {\n            const pos = i * ratio;\n            const index = Math.floor(pos);\n            const fraction = pos - index;\n            \n            if (index + 1 < audioData.length) {\n                // Interpolation linéaire simple\n                result[i] = audioData[index] * (1 - fraction) + audioData[index + 1] * fraction;\n            } else {\n                result[i] = audioData[index];\n            }\n        }\n        \n        return result;\n    }\n}\n\n//# sourceURL=webpack://voice-test-html/./js/audio/recorder.js?");

/***/ }),

/***/ "./js/audio/vad.js":
/*!*************************!*\
  !*** ./js/audio/vad.js ***!
  \*************************/
/***/ ((__unused_webpack_module, __webpack_exports__, __webpack_require__) => {

eval("__webpack_require__.r(__webpack_exports__);\n/* harmony export */ __webpack_require__.d(__webpack_exports__, {\n/* harmony export */   VoiceDetector: () => (/* binding */ VoiceDetector)\n/* harmony export */ });\n/**\n * Gestion de la détection d'activité vocale\n */\nclass VoiceDetector {\n    /**\n     * @param {Object} callbacks - Fonctions de callback\n     */\n    constructor(callbacks = {}) {\n        this.vad = null;\n        this.mediaStream = null;\n        this.callbacks = callbacks;\n        this.listening = false;\n        this.muted = false;\n    }\n\n    /**\n     * Initialise le détecteur d'activité vocale\n     * @returns {Promise<boolean>} Succès ou échec de l'initialisation\n     */\n    async initialize() {\n        try {\n            this.vad = await vad.MicVAD.new({\n                onSpeechStart: () => {\n                    console.log(\"Parole détectée\");\n                    if (this.callbacks.onSpeechStart) {\n                        this.callbacks.onSpeechStart();\n                    }\n                },\n                onSpeechEnd: (audio) => {\n                    console.log(\"Fin de parole\");\n                    if (!this.muted && this.callbacks.onSpeechEnd) {\n                        this.callbacks.onSpeechEnd(audio);\n                    }\n                },\n                onVADMisfire: () => {\n                    console.log(\"Fausse détection\");\n                    if (this.callbacks.onVADMisfire) {\n                        this.callbacks.onVADMisfire();\n                    }\n                }\n            });\n            \n            this.mediaStream = await navigator.mediaDevices.getUserMedia({ audio: true });\n            return true;\n        } catch (error) {\n            console.error(\"Erreur lors de l'initialisation du VAD:\", error);\n            if (this.callbacks.onError) {\n                this.callbacks.onError(error);\n            }\n            return false;\n        }\n    }\n\n    /**\n     * Démarre la détection vocale\n     */\n    start() {\n        if (this.vad) {\n            this.vad.start();\n            this.listening = true;\n        }\n    }\n\n    /**\n     * Met en pause la détection vocale\n     */\n    pause() {\n        if (this.vad) {\n            this.vad.pause();\n            this.listening = false;\n        }\n    }\n\n    /**\n     * Arrête complètement la détection et libère les ressources\n     */\n    stop() {\n        this.pause();\n        if (this.mediaStream) {\n            this.mediaStream.getTracks().forEach(track => track.stop());\n        }\n        this.listening = false;\n    }\n\n    /**\n     * Active ou désactive le microphone\n     * @param {boolean} muted - État de la sourdine\n     */\n    setMute(muted) {\n        this.muted = muted;\n        if (this.mediaStream) {\n            this.mediaStream.getAudioTracks().forEach(track => {\n                track.enabled = !muted;\n            });\n        }\n    }\n\n    /**\n     * Vérifie si la détection est active\n     * @returns {boolean} État d'écoute\n     */\n    isListening() {\n        return this.listening;\n    }\n\n    /**\n     * Vérifie si le micro est en sourdine\n     * @returns {boolean} État de la sourdine\n     */\n    isMuted() {\n        return this.muted;\n    }\n}\n\n//# sourceURL=webpack://voice-test-html/./js/audio/vad.js?");

/***/ }),

/***/ "./js/main.js":
/*!********************!*\
  !*** ./js/main.js ***!
  \********************/
/***/ ((__unused_webpack_module, __webpack_exports__, __webpack_require__) => {

eval("__webpack_require__.r(__webpack_exports__);\n/* harmony import */ var _audio_vad_js__WEBPACK_IMPORTED_MODULE_0__ = __webpack_require__(/*! ./audio/vad.js */ \"./js/audio/vad.js\");\n/* harmony import */ var _api_transcription_js__WEBPACK_IMPORTED_MODULE_1__ = __webpack_require__(/*! ./api/transcription.js */ \"./js/api/transcription.js\");\n/* harmony import */ var _api_chat_js__WEBPACK_IMPORTED_MODULE_2__ = __webpack_require__(/*! ./api/chat.js */ \"./js/api/chat.js\");\n/* harmony import */ var _api_tts_js__WEBPACK_IMPORTED_MODULE_3__ = __webpack_require__(/*! ./api/tts.js */ \"./js/api/tts.js\");\n/* harmony import */ var _ui_interface_js__WEBPACK_IMPORTED_MODULE_4__ = __webpack_require__(/*! ./ui/interface.js */ \"./js/ui/interface.js\");\n/* harmony import */ var _ui_conversation_js__WEBPACK_IMPORTED_MODULE_5__ = __webpack_require__(/*! ./ui/conversation.js */ \"./js/ui/conversation.js\");\n/* harmony import */ var _utils_storage_js__WEBPACK_IMPORTED_MODULE_6__ = __webpack_require__(/*! ./utils/storage.js */ \"./js/utils/storage.js\");\n/* harmony import */ var _ui_slider_init_js__WEBPACK_IMPORTED_MODULE_7__ = __webpack_require__(/*! ./ui/slider-init.js */ \"./js/ui/slider-init.js\");\n/* harmony import */ var _audio_recorder_js__WEBPACK_IMPORTED_MODULE_8__ = __webpack_require__(/*! ./audio/recorder.js */ \"./js/audio/recorder.js\");\n\n\n\n\n\n\n\n\n\n\n\ndocument.addEventListener('DOMContentLoaded', () => {\n    // Initialisation des modules\n    const storage = new _utils_storage_js__WEBPACK_IMPORTED_MODULE_6__.StorageManager();\n    const ui = new _ui_interface_js__WEBPACK_IMPORTED_MODULE_4__.UIManager();\n    const conversation = new _ui_conversation_js__WEBPACK_IMPORTED_MODULE_5__.ConversationManager(document.getElementById('conversation-list'));\n    const transcriptionService = new _api_transcription_js__WEBPACK_IMPORTED_MODULE_1__.TranscriptionService();\n    const chatService = new _api_chat_js__WEBPACK_IMPORTED_MODULE_2__.ChatService();\n    const ttsService = new _api_tts_js__WEBPACK_IMPORTED_MODULE_3__.TTSService();\n    \n    // Initialiser le slider\n    const slider = (0,_ui_slider_init_js__WEBPACK_IMPORTED_MODULE_7__.initSlider)();\n    \n    // Variables d'état\n    let voiceDetector = null;\n    let segments = [];\n    let processing = false;\n    let wasInterrupted = false;\n    let isInterrupting = false;\n    let messageCounter = 0; // Compteur global pour les identifiants de message\n    let audioRecorder = null;\n\n    // Configurer les callbacks du ttsService\n    ttsService.onSpeakingStarted = () => {\n        console.log(\"TTS a commencé à parler\");\n        // Réactiver la détection vocale pendant que le TTS parle\n        if (voiceDetector) {\n            voiceDetector.start();\n            ui.updateUI(true, false);\n        }\n    };\n    \n    ttsService.onSpeakingEnded = () => {\n        console.log(\"TTS a terminé de parler\");\n        // Le TTS s'est terminé normalement\n    };\n    \n    ttsService.onSpeakingInterrupted = () => {\n        console.log(\"TTS a été interrompu\");\n        wasInterrupted = true;\n        // Marquer visuellement que l'assistant a été interrompu\n        // conversation.addInfoMessage(\"Assistant interrompu\", \"warning\");\n    };\n    \n    // Charger la clé API depuis le stockage local\n    const savedKey = storage.getItem('groq-api-key');\n    if (savedKey) {\n        ui.setApiKey(savedKey);\n    }\n    \n    // Sauvegarder la clé API quand elle change\n    document.getElementById('api-key').addEventListener('change', () => {\n        storage.setItem('groq-api-key', ui.getApiKey());\n    });\n    \n    // Gestionnaire pour le bouton de démarrage\n    document.getElementById('start-button').addEventListener('click', async () => {\n        try {\n            // Initialiser le détecteur vocal\n            voiceDetector = new _audio_vad_js__WEBPACK_IMPORTED_MODULE_0__.VoiceDetector({\n                onSpeechStart: () => {\n                    console.log(\"Parole détectée\");\n                    \n                    // Vérifier si le TTS est en cours et l'interrompre IMMÉDIATEMENT\n                    if (ttsService.isSpeaking) {\n                        console.log(\"Interruption du TTS\");\n                        isInterrupting = true;\n                        ttsService.cancel();\n                        \n                        // Ajouter cette ligne pour signaler visuellement l'interruption immédiatement\n                        // conversation.addInfoMessage(\"Assistant interrompu\", \"warning\");\n                        \n                        // Forcer la mise à jour de l'UI immédiatement\n                        ui.updateUI(true, true);\n                        \n                        // IMPORTANT: Annuler aussi les requêtes en cours immédiatement\n                        transcriptionService.cancel();\n                        chatService.cancel();\n                        \n                        // Réinitialiser immédiatement l'état de traitement\n                        processing = false;\n                    }\n                    \n                    if (!voiceDetector.isMuted()) {\n                        ui.updateUI(true, true);\n                    }\n                },\n                onSpeechEnd: (audio) => {\n                    console.log(\"Fin de parole, audio length:\", audio.length);\n                    \n                    // Si le micro n'est pas en sourdine, traiter l'audio\n                    if (!voiceDetector.isMuted()) {\n                        ui.updateUI(true, false);\n                        \n                        // Traiter l'audio même en cas d'interruption\n                        if (audio && audio.length >= 100) {\n                            processAudioSegment(audio);\n                        } else {\n                            console.warn(\"Segment audio trop court ou invalide\", audio?.length);\n                        }\n                        \n                        // Réinitialiser le flag d'interruption après traitement\n                        isInterrupting = false;\n                    }\n                },\n                \n                onError: (error) => {\n                    console.error(\"Erreur de détection vocale:\", error);\n                    conversation.addInfoMessage(`Erreur: ${error.message}`, 'error');\n                }\n            });\n            \n            // Initialisation du détecteur\n            const success = await voiceDetector.initialize();\n            if (success) {\n                voiceDetector.start();\n                ui.updateUI(true, false);\n                conversation.addInfoMessage(\"Détection vocale démarrée\", 'success');\n            } else {\n                throw new Error(\"Échec de l'initialisation de la détection vocale\");\n            }\n        } catch (error) {\n            console.error(\"Erreur lors de l'initialisation:\", error);\n            conversation.addInfoMessage(`Erreur d'initialisation: ${error.message}`, 'error');\n        }\n    });\n    \n    // Gestionnaire pour le bouton d'arrêt\n    document.getElementById('stop-button').addEventListener('click', () => {\n        stopEverything();\n    });\n    \n    // Gestionnaire pour le bouton de sourdine\n    document.getElementById('mute-button').addEventListener('click', () => {\n        if (voiceDetector) {\n            voiceDetector.setMute(!voiceDetector.isMuted());\n            ui.updateMuteState(voiceDetector.isMuted());\n        }\n    });\n    \n    // Gestion des boutons d'enregistrement manuel\n    const recordButton = document.getElementById('record-button');\n    const stopRecordButton = document.getElementById('stop-record-button');\n    \n    // Cacher initialement le bouton stop d'enregistrement\n    if (stopRecordButton) {\n        stopRecordButton.style.display = 'none';\n    }\n\n    if (recordButton && stopRecordButton) {\n        recordButton.addEventListener('click', async () => {\n            // Initialiser l'enregistreur si ce n'est pas déjà fait\n            if (!audioRecorder) {\n                audioRecorder = new _audio_recorder_js__WEBPACK_IMPORTED_MODULE_8__.AudioRecorder({\n                    onRecordingStart: () => {\n                        console.log(\"Enregistrement manuel démarré\");\n                        \n                        // Basculer l'affichage des boutons\n                        recordButton.style.display = 'none';\n                        stopRecordButton.style.display = 'flex';\n                        \n                        // Mettre à jour l'UI\n                        recordButton.disabled = true;\n                        recordButton.classList.add('bg-gray-300', 'text-gray-500', 'cursor-not-allowed');\n                        recordButton.classList.remove('bg-indigo-700', 'text-white', 'hover:bg-indigo-800');\n                        \n                        stopRecordButton.disabled = false;\n                        stopRecordButton.classList.add('bg-red-500', 'text-white', 'hover:bg-red-600');\n                        stopRecordButton.classList.remove('bg-slate-200', 'text-slate-400', 'cursor-not-allowed');\n                        \n                        // Ajouter un message d'information\n                        conversation.addInfoMessage(\"Enregistrement en cours...\", \"info\");\n                    },\n                    onRecordingStop: () => {\n                        console.log(\"Enregistrement manuel arrêté\");\n                        \n                        // Basculer l'affichage des boutons\n                        stopRecordButton.style.display = 'none';\n                        recordButton.style.display = 'flex';\n                        \n                        // Mettre à jour l'UI\n                        recordButton.disabled = false;\n                        recordButton.classList.remove('bg-gray-300', 'text-gray-500', 'cursor-not-allowed');\n                        recordButton.classList.add('bg-indigo-700', 'text-white', 'hover:bg-indigo-800');\n                        \n                        stopRecordButton.disabled = true;\n                        stopRecordButton.classList.remove('bg-red-500', 'text-white', 'hover:bg-red-600');\n                        stopRecordButton.classList.add('bg-slate-200', 'text-slate-400', 'cursor-not-allowed');\n                        \n                        // S'assurer que la détection vocale ne démarre pas automatiquement\n                        if (voiceDetector && voiceDetector.isListening()) {\n                            console.log(\"Arrêt de la détection vocale après enregistrement\");\n                            voiceDetector.stop();\n                            ui.updateUI(false);\n                        }\n                    },\n                    onRecordingComplete: async (audioData) => {\n                        console.log(\"Enregistrement terminé, traitement en cours\");\n                        \n                        // Traiter l'audio enregistré de la même manière que les segments VAD\n                        processAudioSegment(audioData);\n                    },\n                    onError: (error) => {\n                        console.error(\"Erreur d'enregistrement:\", error);\n                        conversation.addInfoMessage(`Erreur: ${error.message}`, 'error');\n                    }\n                });\n                \n                // Initialiser l'enregistreur\n                const success = await audioRecorder.initialize();\n                if (!success) {\n                    console.error(\"Échec de l'initialisation de l'enregistreur\");\n                    conversation.addInfoMessage(\"Impossible d'initialiser l'enregistreur\", 'error');\n                    return;\n                }\n            }\n            \n            // Démarrer l'enregistrement\n            audioRecorder.startRecording();\n        });\n        \n        stopRecordButton.addEventListener('click', () => {\n            if (audioRecorder && audioRecorder.isRecording()) {\n                audioRecorder.stopRecording();\n                \n                // Force l'état visuel correct pour les boutons de détection vocale\n                const startButton = document.getElementById('start-button');\n                const stopButton = document.getElementById('stop-button');\n                \n                // Forcer l'affichage correct quoi qu'il arrive\n                startButton.style.display = 'flex';\n                stopButton.style.display = 'none';\n            }\n        });\n    }\n    \n    // Ajout de la gestion de l'input texte\n    const textForm = document.getElementById('text-input-form');\n    if (textForm) {\n        textForm.addEventListener('submit', async (e) => {\n            e.preventDefault();\n            \n            const textInput = document.getElementById('text-input');\n            const message = textInput.value.trim();\n            if (!message) return;\n            \n            try {\n                // Récupérer la clé API\n                const apiKey = ui.getApiKey();\n                if (!apiKey) {\n                    throw new Error(\"Aucune clé API fournie\");\n                }\n                \n                // Incrémenter le compteur global et créer un ID unique\n                const messageId = ++messageCounter;\n                \n                // Ajouter le message à l'interface\n                conversation.addUserTextMessage(messageId, message);\n                \n                // Vider l'input\n                textInput.value = '';\n                \n                // Désactiver l'input pendant le traitement\n                textInput.disabled = true;\n                document.getElementById('send-text-button').disabled = true;\n                \n                // Ajouter un message assistant en attente\n                conversation.addPendingAssistantMessage(messageId);\n                \n                // Générer une réponse\n                const response = await chatService.sendMessage(message, apiKey);\n                console.log(\"Réponse du LLM:\", response);\n                \n                // Mettre à jour le message assistant avec la réponse\n                conversation.updateAssistantMessage(messageId, response);\n                \n                // Synthétiser la réponse en audio si souhaité\n                if (voiceDetector && voiceDetector.isListening()) {\n                    try {\n                        await ttsService.speak(response);\n                    } catch (ttsError) {\n                        console.error(\"Erreur TTS ignorée:\", ttsError);\n                    }\n                }\n                \n            } catch (error) {\n                console.error(\"Erreur lors du traitement du texte:\", error);\n                conversation.addInfoMessage(`Erreur: ${error.message}`, 'error');\n            } finally {\n                // Réactiver l'input\n                const textInput = document.getElementById('text-input');\n                if (textInput) {\n                    textInput.disabled = false;\n                    textInput.focus();\n                }\n                \n                const sendButton = document.getElementById('send-text-button');\n                if (sendButton) {\n                    sendButton.disabled = false;\n                }\n            }\n        });\n    }\n    \n    /**\n     * Traite un segment audio\n     * @param {Float32Array} audioData - Données audio\n     */\n    async function processAudioSegment(audioData) {\n        // Vérifier si l'audio est valide\n        if (!audioData || audioData.length < 100) {\n            console.warn(\"Segment audio trop court ou invalide\", audioData?.length);\n            return;\n        }\n        \n        // Ajouter le segment à la liste\n        segments.push(audioData);\n        \n        // Incrémenter le compteur global et créer un ID unique\n        const segmentId = ++messageCounter;\n        \n        ui.addSegment(segments.length, audioData.length / 16000);\n        \n        try {\n            // Signaler que le traitement commence\n            processing = true;\n            ui.updateUI(true, false, true);\n            \n            // Mettre en pause la détection vocale pendant le traitement\n            if (voiceDetector) {\n                voiceDetector.pause();\n            }\n            \n            // Ajouter un message utilisateur en attente\n            conversation.addPendingUserMessage(segmentId);\n            \n            // Transcription\n            const apiKey = ui.getApiKey();\n            if (!apiKey) {\n                throw new Error(\"Aucune clé API fournie\");\n            }\n            \n            const transcription = await transcriptionService.transcribe(audioData, apiKey);\n            console.log(\"Transcription:\", transcription);\n            \n            // Mettre à jour le message utilisateur avec la transcription\n            conversation.updateUserMessage(segmentId, transcription);\n            \n            // Ajouter un message assistant en attente\n            conversation.addPendingAssistantMessage(segmentId);\n            \n            // Générer une réponse\n            const response = await chatService.sendMessage(transcription, apiKey);\n            console.log(\"Réponse du LLM:\", response);\n            \n            // Mettre à jour le message assistant avec la réponse IMMÉDIATEMENT\n            // C'est crucial que cette ligne s'exécute avant la tentative de TTS\n            conversation.updateAssistantMessage(segmentId, response);\n            \n            // Le traitement API est terminé\n            // Maintenant tenter de jouer le TTS, mais ne pas bloquer si ça échoue\n            wasInterrupted = false; // Réinitialiser le drapeau d'interruption\n            try {\n                await ttsService.speak(response);\n            } catch (ttsError) {\n                console.error(\"Erreur lors de la synthèse vocale:\", ttsError);\n                // Ne pas bloquer en cas d'erreur TTS\n            }\n            \n        } catch (error) {\n            // Vérifier si l'erreur est due à une annulation volontaire\n            if (error.name === 'AbortError') {\n                console.log(\"Requête annulée volontairement\");\n                return;\n            }\n            \n            console.error(\"Erreur de traitement:\", error);\n            conversation.addInfoMessage(`Erreur: ${error.message}`, 'error');\n        } finally {\n            // Fin du traitement\n            processing = false;\n            \n            // Si le processus n'a pas été interrompu et que la détection est toujours active\n            if (!wasInterrupted && voiceDetector && voiceDetector.isListening()) {\n                ui.updateUI(true, false);\n            }\n        }\n    }\n    \n    /**\n     * Arrête complètement tout\n     */\n    function stopEverything() {\n        // Arrêter les détections et enregistrements\n        if (voiceDetector) {\n            voiceDetector.stop();\n        }\n        \n        // Arrêter l'enregistreur manuel\n        if (audioRecorder) {\n            if (audioRecorder.isRecording()) {\n                audioRecorder.stopRecording();\n            }\n            audioRecorder.dispose();\n            audioRecorder = null;\n        }\n        \n        // Annuler les requêtes en cours\n        transcriptionService.cancel();\n        chatService.cancel();\n        ttsService.cancel();\n        \n        // Réinitialiser l'état\n        processing = false;\n        wasInterrupted = false;\n        isInterrupting = false;\n        \n        // Mettre à jour l'UI\n        ui.updateUI(false);\n        \n        // Réinitialiser l'affichage des boutons d'enregistrement si nécessaire\n        if (recordButton && stopRecordButton) {\n            stopRecordButton.style.display = 'none';\n            recordButton.style.display = 'flex';\n            \n            recordButton.disabled = false;\n            recordButton.classList.remove('bg-gray-300', 'text-gray-500', 'cursor-not-allowed');\n            recordButton.classList.add('bg-indigo-700', 'text-white', 'hover:bg-indigo-800');\n            \n            stopRecordButton.disabled = true;\n            stopRecordButton.classList.remove('bg-red-500', 'text-white', 'hover:bg-red-600');\n            stopRecordButton.classList.add('bg-slate-200', 'text-slate-400', 'cursor-not-allowed');\n        }\n        \n        // Informer l'utilisateur\n        conversation.addInfoMessage('Session arrêtée', 'info');\n    }\n    \n    // Gestionnaire pour le bouton de réinitialisation dans le menu latéral\n    const resetButton = document.querySelector('button.bg-emerald-50');\n    if (resetButton) {\n        resetButton.addEventListener('click', () => {\n            // Arrêter tout\n            stopEverything();\n            \n            // Vider la conversation\n            conversation.clearConversation();\n            \n            // Vider les segments\n            segments = [];\n            ui.resetSegments();\n            \n            // Effacer l'historique du chat\n            chatService.clearHistory();\n            \n            // Réinitialiser le compteur de messages\n            messageCounter = 0;\n            \n            // Informer l'utilisateur\n            conversation.addInfoMessage('Conversation réinitialisée', 'success');\n        });\n    }\n});\n\n//# sourceURL=webpack://voice-test-html/./js/main.js?");

/***/ }),

/***/ "./js/ui/conversation.js":
/*!*******************************!*\
  !*** ./js/ui/conversation.js ***!
  \*******************************/
/***/ ((__unused_webpack_module, __webpack_exports__, __webpack_require__) => {

eval("__webpack_require__.r(__webpack_exports__);\n/* harmony export */ __webpack_require__.d(__webpack_exports__, {\n/* harmony export */   ConversationManager: () => (/* binding */ ConversationManager)\n/* harmony export */ });\n/**\n * Gestionnaire d'affichage de la conversation\n */\nclass ConversationManager {\n    /**\n     * @param {HTMLElement} container - Élément conteneur pour la conversation\n     */\n    constructor(container) {\n        this.container = container;\n    }\n\n    /**\n     * Ajoute un message utilisateur avec statut en attente\n     * @param {number} id - Identifiant du message\n     * @returns {HTMLElement} Élément du message\n     */\n    addPendingUserMessage(id) {\n        const message = document.createElement('div');\n        message.className = 'message user mb-6 flex justify-end';\n        message.id = `message-${id}`;\n        message.innerHTML = `\n            <div class=\"bg-blue-900 text-white p-4 rounded-2xl rounded-tr-none shadow-md max-w-[80%]\">\n                <div><span class=\"inline-block animate-pulse\">Transcription en cours...</span></div>\n                <div class=\"flex justify-end mt-1\">\n                    <span class=\"text-xs text-blue-200\">${new Date().toLocaleTimeString([], {hour: '2-digit', minute:'2-digit'})}</span>\n                </div>\n            </div>\n        `;\n        this.container.appendChild(message);\n        this.scrollToBottom();\n        return message;\n    }\n\n    /**\n     * Met à jour un message utilisateur avec le texte transcrit\n     * @param {number} id - Identifiant du message\n     * @param {string} text - Texte transcrit\n     */\n    updateUserMessage(id, text) {\n        const message = document.getElementById(`message-${id}`);\n        if (message) {\n            const contentDiv = message.querySelector('div div:first-child');\n            contentDiv.innerHTML = ''; // Vider le contenu\n            contentDiv.textContent = text; // Ajouter le nouveau texte\n            contentDiv.classList.remove('animate-pulse');\n        }\n        this.scrollToBottom();\n    }\n\n    /**\n     * Ajoute un message de l'assistant avec statut en attente\n     * @param {number} id - Identifiant du message\n     * @returns {HTMLElement} Élément du message\n     */\n    addPendingAssistantMessage(id) {\n        const message = document.createElement('div');\n        message.className = 'message assistant mb-6 flex justify-start';\n        message.id = `response-${id}`;\n        message.innerHTML = `\n            <div class=\"bg-slate-100 p-4 rounded-2xl rounded-tl-none shadow-sm max-w-[80%] text-slate-800\">\n                <div><span class=\"inline-block animate-pulse\">Génération de la réponse...</span></div>\n                <div class=\"flex justify-start mt-1\">\n                    <span class=\"text-xs text-slate-500\">${new Date().toLocaleTimeString([], {hour: '2-digit', minute:'2-digit'})}</span>\n                </div>\n            </div>\n        `;\n        this.container.appendChild(message);\n        this.scrollToBottom();\n        return message;\n    }\n\n    /**\n     * Met à jour un message de l'assistant avec la réponse\n     * @param {number} id - Identifiant du message\n     * @param {string} text - Texte de la réponse\n     */\n    updateAssistantMessage(id, text) {\n        const message = document.getElementById(`response-${id}`);\n        if (message) {\n            const contentDiv = message.querySelector('div div:first-child');\n            contentDiv.innerHTML = ''; // Vider le contenu\n            contentDiv.textContent = text; // Ajouter le nouveau texte\n            contentDiv.classList.remove('animate-pulse');\n        }\n        this.scrollToBottom();\n    }\n\n    /**\n     * Ajoute un message d'information\n     * @param {string} text - Texte informatif\n     * @param {string} type - Type de message (info, error, success, warning)\n     */\n    addInfoMessage(text, type = 'info') {\n        const colors = {\n            info: 'bg-slate-200 text-slate-800',\n            error: 'bg-red-100 text-red-800',\n            success: 'bg-green-100 text-green-800',\n            warning: 'bg-amber-100 text-amber-800'\n        };\n        \n        const message = document.createElement('div');\n        message.className = `p-3 mb-4 ${colors[type]} rounded-md text-center text-sm max-w-[80%] mx-auto shadow-sm`;\n        message.textContent = text;\n        this.container.appendChild(message);\n        this.scrollToBottom();\n    }\n\n    /**\n     * Fait défiler la conversation jusqu'en bas\n     */\n    scrollToBottom() {\n        this.container.scrollTop = this.container.scrollHeight;\n    }\n\n    /**\n     * Ajoute un message utilisateur provenant de l'input texte\n     * @param {number} id - Identifiant du message\n     * @param {string} text - Texte du message\n     */\n    addUserTextMessage(id, text) {\n        const message = document.createElement('div');\n        message.className = 'message user mb-6 flex justify-end';\n        message.id = `message-${id}`;\n        message.innerHTML = `\n            <div class=\"bg-blue-900 text-white p-4 rounded-2xl rounded-tr-none shadow-md max-w-[80%]\">\n                <p>${text}</p>\n                <div class=\"flex justify-end mt-1\">\n                    <span class=\"text-xs text-blue-200\">${new Date().toLocaleTimeString([], {hour: '2-digit', minute:'2-digit'})}</span>\n                </div>\n            </div>\n        `;\n        this.container.appendChild(message);\n        this.scrollToBottom();\n        return message;\n    }\n\n    /**\n     * Vide la conversation\n     */\n    clearConversation() {\n        this.container.innerHTML = '';\n    }\n}\n\n//# sourceURL=webpack://voice-test-html/./js/ui/conversation.js?");

/***/ }),

/***/ "./js/ui/interface.js":
/*!****************************!*\
  !*** ./js/ui/interface.js ***!
  \****************************/
/***/ ((__unused_webpack_module, __webpack_exports__, __webpack_require__) => {

eval("__webpack_require__.r(__webpack_exports__);\n/* harmony export */ __webpack_require__.d(__webpack_exports__, {\n/* harmony export */   UIManager: () => (/* binding */ UIManager)\n/* harmony export */ });\n/**\n * Gestion de l'interface utilisateur\n */\nclass UIManager {\n    constructor() {\n        // Éléments DOM\n        this.elements = {\n            startButton: document.getElementById('start-button'),\n            stopButton: document.getElementById('stop-button'),\n            muteButton: document.getElementById('mute-button'),\n            statusIndicator: document.getElementById('status-indicator'),\n            statusText: document.getElementById('status-text'),\n            segmentsCount: document.getElementById('segments-count'),\n            segmentsList: document.getElementById('segments-list'),\n            apiKeyInput: document.getElementById('api-key')\n        };\n        \n        // Cacher initialement le bouton stop\n        if (this.elements.stopButton) {\n            this.elements.stopButton.style.display = 'none';\n        }\n    }\n\n    /**\n     * Met à jour l'interface en fonction de l'état\n     * @param {boolean} listening - En écoute ou non\n     * @param {boolean} speaking - Parole détectée\n     * @param {boolean} processing - En cours de traitement\n     */\n    updateUI(listening, speaking = false, processing = false) {\n        if (listening) {\n            // Mode écoute\n            // Cacher le bouton démarrer et montrer le bouton arrêt\n            this.elements.startButton.style.display = 'none';\n            this.elements.stopButton.style.display = 'flex';\n            \n            // Mise à jour des états et styles\n            this.elements.startButton.disabled = true;\n            this.elements.startButton.classList.add('bg-gray-300', 'text-gray-500', 'cursor-not-allowed');\n            this.elements.startButton.classList.remove('bg-blue-800', 'text-white', 'hover:bg-blue-900');\n            \n            this.elements.stopButton.disabled = false;\n            this.elements.stopButton.classList.add('bg-red-500', 'text-white', 'hover:bg-red-600');\n            this.elements.stopButton.classList.remove('bg-slate-200', 'text-slate-400', 'cursor-not-allowed');\n            \n            this.elements.muteButton.disabled = false;\n            this.elements.muteButton.classList.remove('cursor-not-allowed');\n            \n            if (processing) {\n                this.elements.statusText.textContent = 'Traitement en cours...';\n                this.elements.statusIndicator.classList.remove('bg-slate-200', 'bg-yellow-500', 'bg-green-500', 'bg-red-500');\n                this.elements.statusIndicator.classList.add('bg-blue-500');\n            } else {\n                this.elements.statusText.textContent = speaking ? 'Parole détectée' : 'En attente de parole';\n                this.elements.statusIndicator.classList.remove('bg-slate-200', 'bg-blue-500', 'bg-red-500');\n                this.elements.statusIndicator.classList.add(speaking ? 'bg-green-500' : 'bg-yellow-500');\n            }\n        } else {\n            // Mode arrêté\n            // Montrer le bouton démarrer et cacher le bouton arrêt\n            this.elements.startButton.style.display = 'flex';\n            this.elements.stopButton.style.display = 'none';\n            \n            // Mise à jour des états et styles\n            this.elements.startButton.disabled = false;\n            this.elements.startButton.classList.remove('bg-gray-300', 'text-gray-500', 'cursor-not-allowed');\n            this.elements.startButton.classList.add('bg-blue-800', 'text-white', 'hover:bg-blue-900');\n            \n            this.elements.stopButton.disabled = true;\n            this.elements.stopButton.classList.remove('bg-red-500', 'text-white', 'hover:bg-red-600');\n            this.elements.stopButton.classList.add('bg-slate-200', 'text-slate-400', 'cursor-not-allowed');\n            \n            this.elements.muteButton.disabled = true;\n            this.elements.muteButton.classList.add('cursor-not-allowed');\n            \n            this.elements.statusText.textContent = 'En attente de démarrer';\n            this.elements.statusIndicator.classList.remove('bg-green-500', 'bg-yellow-500', 'bg-red-500', 'bg-blue-500');\n            this.elements.statusIndicator.classList.add('bg-slate-200');\n        }\n    }\n\n    /**\n     * Met à jour l'état de la sourdine\n     * @param {boolean} muted - État de la sourdine\n     */\n    updateMuteState(muted) {\n        if (muted) {\n            this.elements.muteButton.innerHTML = '<i class=\"fas fa-microphone mr-2\"></i>';\n            this.elements.muteButton.classList.remove('bg-amber-500', 'hover:bg-amber-600');\n            this.elements.muteButton.classList.add('bg-green-500', 'hover:bg-green-600');\n            this.elements.statusIndicator.classList.remove('bg-yellow-500', 'bg-green-500');\n            this.elements.statusIndicator.classList.add('bg-red-500');\n            this.elements.statusText.textContent = 'Microphone coupé';\n        } else {\n            this.elements.muteButton.innerHTML = '<i class=\"fas fa-microphone-slash mr-2\"></i>';\n            this.elements.muteButton.classList.remove('bg-green-500', 'hover:bg-green-600');\n            this.elements.muteButton.classList.add('bg-amber-500', 'hover:bg-amber-600');\n            this.elements.statusIndicator.classList.remove('bg-red-500');\n            this.elements.statusIndicator.classList.add('bg-yellow-500');\n            this.elements.statusText.textContent = 'En attente de parole';\n        }\n    }\n\n    /**\n     * Ajoute un segment audio à la liste\n     * @param {number} segmentId - ID du segment\n     * @param {number} duration - Durée en secondes\n     */\n    addSegment(segmentId, duration) {\n        this.elements.segmentsCount.textContent = segmentId;\n        \n        const segment = document.createElement('div');\n        segment.className = 'text-sm text-gray-700';\n        segment.textContent = `Segment #${segmentId} - ${Math.round(duration * 100) / 100}s`;\n        this.elements.segmentsList.appendChild(segment);\n    }\n\n    /**\n     * Réinitialise les segments\n     */\n    resetSegments() {\n        this.elements.segmentsCount.textContent = '0';\n        this.elements.segmentsList.innerHTML = '';\n    }\n\n    /**\n     * Récupère la clé API\n     * @returns {string} Clé API\n     */\n    getApiKey() {\n        return this.elements.apiKeyInput.value;\n    }\n\n    /**\n     * Définit la clé API\n     * @param {string} key - Clé API\n     */\n    setApiKey(key) {\n        this.elements.apiKeyInput.value = key;\n    }\n}\n\n//# sourceURL=webpack://voice-test-html/./js/ui/interface.js?");

/***/ }),

/***/ "./js/ui/slider-init.js":
/*!******************************!*\
  !*** ./js/ui/slider-init.js ***!
  \******************************/
/***/ ((__unused_webpack_module, __webpack_exports__, __webpack_require__) => {

eval("__webpack_require__.r(__webpack_exports__);\n/* harmony export */ __webpack_require__.d(__webpack_exports__, {\n/* harmony export */   initSlider: () => (/* binding */ initSlider)\n/* harmony export */ });\nfunction initSlider() {\n    if (typeof Swiper === 'function') {\n        const swiper = new Swiper('.swiper-container', {\n            // Options de base\n            direction: 'horizontal',\n            loop: true,\n            // autoplay: {\n            //     delay: 5000,\n            //     disableOnInteraction: false,\n            // },\n            \n            // Important : paramètres pour n'afficher qu'un seul slide\n            slidesPerView: 1,\n            spaceBetween: 0,\n            \n            // Empêcher l'effet de débordement\n            width: null,  // Laisser Swiper calculer automatiquement\n            \n            // Configuration optimisée pour mobile\n            touchEventsTarget: 'container',\n            touchRatio: 1,\n            touchAngle: 45,\n            simulateTouch: true,\n            shortSwipes: true,\n            grabCursor: true,\n            \n            // Pagination\n            pagination: {\n                el: '.swiper-pagination',\n                clickable: true,\n            },\n            \n            // Effets\n            effect: 'slide',\n            speed: 400,\n        });\n        \n        return swiper;\n    } else {\n        console.error(\"La bibliothèque Swiper n'est pas chargée.\");\n        return null;\n    }\n}\n\n//# sourceURL=webpack://voice-test-html/./js/ui/slider-init.js?");

/***/ }),

/***/ "./js/utils/storage.js":
/*!*****************************!*\
  !*** ./js/utils/storage.js ***!
  \*****************************/
/***/ ((__unused_webpack_module, __webpack_exports__, __webpack_require__) => {

eval("__webpack_require__.r(__webpack_exports__);\n/* harmony export */ __webpack_require__.d(__webpack_exports__, {\n/* harmony export */   StorageManager: () => (/* binding */ StorageManager)\n/* harmony export */ });\n/**\n * Gestion du stockage local\n */\nclass StorageManager {\n    /**\n     * Sauvegarde une valeur dans le stockage local\n     * @param {string} key - Clé de stockage\n     * @param {string} value - Valeur à stocker\n     */\n    setItem(key, value) {\n        localStorage.setItem(key, value);\n    }\n    \n    /**\n     * Récupère une valeur depuis le stockage local\n     * @param {string} key - Clé de stockage\n     * @returns {string|null} La valeur stockée ou null\n     */\n    getItem(key) {\n        return localStorage.getItem(key);\n    }\n    \n    /**\n     * Supprime une valeur du stockage local\n     * @param {string} key - Clé à supprimer\n     */\n    removeItem(key) {\n        localStorage.removeItem(key);\n    }\n}\n\n//# sourceURL=webpack://voice-test-html/./js/utils/storage.js?");

/***/ })

/******/ 	});
/************************************************************************/
/******/ 	// The module cache
/******/ 	var __webpack_module_cache__ = {};
/******/ 	
/******/ 	// The require function
/******/ 	function __webpack_require__(moduleId) {
/******/ 		// Check if module is in cache
/******/ 		var cachedModule = __webpack_module_cache__[moduleId];
/******/ 		if (cachedModule !== undefined) {
/******/ 			return cachedModule.exports;
/******/ 		}
/******/ 		// Create a new module (and put it into the cache)
/******/ 		var module = __webpack_module_cache__[moduleId] = {
/******/ 			// no module.id needed
/******/ 			// no module.loaded needed
/******/ 			exports: {}
/******/ 		};
/******/ 	
/******/ 		// Execute the module function
/******/ 		__webpack_modules__[moduleId](module, module.exports, __webpack_require__);
/******/ 	
/******/ 		// Return the exports of the module
/******/ 		return module.exports;
/******/ 	}
/******/ 	
/************************************************************************/
/******/ 	/* webpack/runtime/define property getters */
/******/ 	(() => {
/******/ 		// define getter functions for harmony exports
/******/ 		__webpack_require__.d = (exports, definition) => {
/******/ 			for(var key in definition) {
/******/ 				if(__webpack_require__.o(definition, key) && !__webpack_require__.o(exports, key)) {
/******/ 					Object.defineProperty(exports, key, { enumerable: true, get: definition[key] });
/******/ 				}
/******/ 			}
/******/ 		};
/******/ 	})();
/******/ 	
/******/ 	/* webpack/runtime/hasOwnProperty shorthand */
/******/ 	(() => {
/******/ 		__webpack_require__.o = (obj, prop) => (Object.prototype.hasOwnProperty.call(obj, prop))
/******/ 	})();
/******/ 	
/******/ 	/* webpack/runtime/make namespace object */
/******/ 	(() => {
/******/ 		// define __esModule on exports
/******/ 		__webpack_require__.r = (exports) => {
/******/ 			if(typeof Symbol !== 'undefined' && Symbol.toStringTag) {
/******/ 				Object.defineProperty(exports, Symbol.toStringTag, { value: 'Module' });
/******/ 			}
/******/ 			Object.defineProperty(exports, '__esModule', { value: true });
/******/ 		};
/******/ 	})();
/******/ 	
/************************************************************************/
/******/ 	
/******/ 	// startup
/******/ 	// Load entry module and return exports
/******/ 	// This entry module can't be inlined because the eval devtool is used.
/******/ 	var __webpack_exports__ = __webpack_require__("./js/main.js");
/******/ 	
/******/ })()
;